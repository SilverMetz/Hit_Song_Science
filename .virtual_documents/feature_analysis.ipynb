%pip install pandas
%pip install matplotlib
%pip install seaborn
%pip install scikit-learn


import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


#df=pd.read_csv ("Dataset/complete.csv")
#not useful


dfs = []
for f in ['00', '10']:
    df=pd.read_csv(f"./Dataset/Songs_1960s_to_2010s/dataset-of-{f}s.csv")
    dfs.append(df)
df_merged = pd.concat(dfs)


df_merged.groupby(['artist', 'target'], as_index=False)['uri'].count().sort_values('uri', ascending=False)








%pip install pandas
%pip install matplotlib
%pip install seaborn
%pip install scipy


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pylab as plt





dfs = []
for f in ['00', '10']:
    df=pd.read_csv(f"./Dataset/Songs_1960s_to_2010s/dataset-of-{f}s.csv")
    dfs.append(df)
df_merged = pd.concat(dfs)
df_with_numerical_features = df_merged[['danceability', 'energy', 'key', 'loudness',
       'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',
       'valence', 'tempo', 'duration_ms', 'time_signature', 'chorus_hit',
       'sections', 'target']]








df_merged.describe()


_ = df_merged.hist()





_ = df_merged.groupby('target').hist()





for feature in df_with_numerical_features.columns:
    if feature == 'target':
        continue
    print(feature)
    _ = df_merged.groupby('target')[feature].hist(alpha=0.5)
    plt.show()





sns.heatmap(df_with_numerical_features.corr())


_ = pd.plotting.scatter_matrix(df_merged, alpha=0.2, figsize=(16, 16), diagonal='kde')





_ = sns.pairplot(df_with_numerical_features)





from sklearn import tree #for now we are using a decision tree

clf = tree.DecisionTreeClassifier()

X = df_with_numerical_features[df_with_numerical_features.columns[0:-1]] #This is what the predictions are based on (the values that influence the outcome)
Y = df_with_numerical_features['target'] #This is the target value. In our case, it's either 1 or 0 and the model wil predict it based on the X-values.

from sklearn.model_selection import train_test_split #We're going to use this to split the dataset into two parts: one to train the model and one to test it.

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30) #Now we have an X and a Y for training and for testing, the training set is 70% of the original dataset, the testing part 30%.

clf = clf.fit(X_train, Y_train) #Now we train the model with 70% of the used dataset.


predictions = clf.predict(X_test) #Now let's see what the model is going to predict if we feed it the remaining 30%.


prediction_df = pd.DataFrame(predictions, columns=['predictions']) #Now we create a dataframe of all the predictions the model makes based on it's training and the test set.
prediction_df['target'] = list(Y_test) #Now we add in a column of the actual target value (not the predicted one), this way we can test the accuracy.
prediction_df['equal'] = prediction_df['predictions'] == prediction_df['target'] #Now we add in a column that tells us if the prediction is equal to the actual target value (now we can test the accuracy).


prediction_df.groupby('equal').agg(cnt=('target', 'count')) #This way we can see how many true and false predictions were made. That's what agg does. It basically counts all 1s and 0s in the equal column, shows the sums of the true and the false and removes the other columns.


prediction_df['predictions'] = 0 #Now we are going to make all the predictions 0 to calculate the baseline.
prediction_df['equal'] = prediction_df['predictions'] == prediction_df['target']
prediction_df.groupby('equal').agg(cnt=('target', 'count'))


1825 / len(prediction_df) #This calculates the baseline accuracy of this model.


2807 / len(prediction_df) #This calculates the accuracy of the model by dividing the true by the length of the entire list.








plt.figure(figsize=(30, 20)) 

_ = tree.plot_tree(clf)



