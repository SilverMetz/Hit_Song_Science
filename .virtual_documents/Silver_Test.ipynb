%pip install pandas
%pip install matplotlib
%pip install seaborn
%pip install scipy
%pip install scikit-learn


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pylab as plt
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


df = pd.read_csv("song_data.csv")
df_num = df[['song_popularity', 'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode', 'speechiness', 'tempo', 'time_signature', 'audio_valence']]

df_num['hit'] = (df_num['song_popularity']>80).astype(int)
df_num.drop(columns='song_popularity', inplace=True)


df


#Added the dataset, made variable with dataset, now I can make some graphs with it.


#Unfortunately, no artist names are given. So we cannot check if specific artists have been overrepresented in the set (so it doesn't contain 6000 Bach songs)


#Now I'm going to check the distribution of the feature values.


df_num.describe()


#Normalizing comes later


#Now let's try to make a histogram of all the features, first not grouped by the popularity, afterwards grouped by popularity. (oops. song_popularity is a range)


_ = df_num.hist()


_ = df_num.groupby('hit').hist()


#Scatterplot time...


_ = sns.heatmap(df_num.corr())


_ = sns.pairplot(df_num)


from sklearn import tree

clf = tree.DecisionTreeClassifier()
X = df_num[df_num.columns[0:-1]]
Y = df_num['hit']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)

clf = clf.fit(X_train, y_train)


preditions = clf.predict(X_test)


prediction_df = pd.DataFrame(list(preditions), columns=['predictions'])
prediction_df['target'] = list(y_test)

import sklearn.metrics

accuracy = sklearn.metrics.accuracy_score(prediction_df['predictions'], prediction_df['target'])
precision = sklearn.metrics.precision_score(prediction_df['predictions'], prediction_df['target'])
recall = sklearn.metrics.recall_score(prediction_df['predictions'], prediction_df['target'])
f1 = sklearn.metrics.f1_score(prediction_df['predictions'], prediction_df['target'])

prediction_df['predictions'] = 0

baseline_accuracy = sklearn.metrics.accuracy_score(prediction_df['predictions'], prediction_df['target'])
baseline_precision = sklearn.metrics.precision_score(prediction_df['predictions'], prediction_df['target'])
baseline_recall = sklearn.metrics.recall_score(prediction_df['predictions'], prediction_df['target'])
baseline_f1 = sklearn.metrics.f1_score(prediction_df['predictions'], prediction_df['target'])


print("The baseline accuracy is ", baseline_accuracy)
print("The baseline precision is ", baseline_precision)
print("The baseline recall is ", baseline_recall)
print("The baseline f1 score is ", baseline_f1)

print("")

print("The accuracy is ", accuracy)
print("The precision is ", precision)
print("The recall is ", recall)
print("The f1 score is ", f1)


#prediction_df['target'] = list(y_test)
#prediction_df['predictions'] = 0
#prediction_df['equal'] = prediction_df['target'] == prediction_df['predictions']


# , as_index=False
#prediction_df.groupby('equal').agg(cnt=('target', 'count'))


#5183 / len(prediction_df)


#5411 / len(prediction_df)


plt.figure(figsize=(30,20))

_= tree.plot_tree(clf)


feature_importance_df = pd.DataFrame(list(zip(clf.feature_importances_, df_num.columns[:-1])), columns=['importance', 'features'])
feature_importance_df.sort_values('importance', ascending=False)









